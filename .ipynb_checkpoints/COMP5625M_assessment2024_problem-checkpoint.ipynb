{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LR4bovYL4CJz"
   },
   "source": [
    "## COMP5625M Practical Assessment - Deep Learning [100 Marks]\n",
    "\n",
    "\n",
    "<div class=\"logos\"><img src=\"Comp5625M_logo.jpg\" width=\"220px\" align=\"right\"></div>\n",
    "\n",
    "This assessment is divided into two parts:\n",
    "> 1. Image classification using DNN and CNN [70 Marks]\n",
    "> 2. Use of RNN to predict texts for image captioning [30 Marks]\n",
    "\n",
    "The maximum number of marks for each part is shown in the section headers. As indicated in the main heading above, the overall assessment carries a maximum of 100 marks.\n",
    "\n",
    "This summative assessment is weighted 50% of the final grade for the module.\n",
    "\n",
    "\n",
    "### Motivation \n",
    "\n",
    "Through this coursework, you will:\n",
    "\n",
    "> 1. Understand and implement your first deep neural network and convolutional neural network (CNN) and see how these can be used for classification problem \n",
    "> 2. Practice building, evaluating, and finetuning your CNN on an image dataset from development to testing stage. \n",
    "> 3. You will learn to tackle overfitting problem using strategies such as data augmentation and drop out.\n",
    "> 4. Compare your model performance and accuracy with others, such as the leaderboard on Kaggle\n",
    "> 5. Use RNNs to predict the caption of an image from established word vocabularies\n",
    "> 6. Understand and visualise text predictions for a given image.\n",
    "\n",
    "\n",
    "### Setup and resources \n",
    "\n",
    "You must work using this provided template notebook.\n",
    "\n",
    "Having a GPU will speed up the training process. See the provided document on Minerva about setting up a working environment for various ways to access a GPU.\n",
    "\n",
    "Please implement the coursework using **Python and PyTorch**, and refer to the notebooks and exercises provided.\n",
    "\n",
    "\n",
    "### Submission\n",
    "\n",
    "Please submit the following:\n",
    "\n",
    "> 1. Your completed Jupyter notebook file, without removing anything in the template, in **.ipynb format.**\n",
    "> 2. The **.html version** of your notebook; File > Download as > HTML (.html). Check that all cells have been run and all outputs (including all graphs you would like to be marked) displayed in the .html for marking.\n",
    "> 3. Your selected image from section 2.4.2 \"Failure analysis\"\n",
    "\n",
    "Final note:\n",
    "\n",
    "> **Please display everything that you would like to be marked. Under each section, put the relevant code containing your solution. You may re-use functions you defined previously, but any new code must be in the relevant section.** Feel free to add as many code cells as you need under each section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Your student username (for example, ```sc15jb```): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> double click to respond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your full name: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> double click to respond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I: Image Classification [70 marks]\n",
    "\n",
    "#### Dataset\n",
    "This coursework will use a subset of images from Tiny ImageNet, which is a subset of the [ImageNet dataset](https://www.image-net.org/update-mar-11-2021.php). Our subset of Tiny ImageNet contains **30 different categories**, we will refer to it as TinyImageNet30. The training set has 450 resized images (64x64 pixels) for each category (13,500 images in total). You can download the training and test set from the Kaggle website:\n",
    "\n",
    ">[Direct access of data is possible by clicking here, please use your university email to access this](https://leeds365-my.sharepoint.com/:u:/g/personal/scssali_leeds_ac_uk/ESF87mN6kelIkjdISkaRow8BublW27jB-P8eWV6Rr4rxtw?e=SPASDB)\n",
    "\n",
    ">[To submit your results on the Kaggle competition. You can also access data here](https://www.kaggle.com/t/9105198471a3490d9057026d27d8a711)\n",
    "\n",
    "To access the dataset, you will need an account on the Kaggle website. Even if you have an existing Kaggle account, please carefully adhere to these instructions, or we may not be able to locate your entries:\n",
    "\n",
    "> 1. Use your **university email** to register a new account.\n",
    "> 2. Set your **Kaggle account NAME** to your university username, for example, ``sc15jb`` (see the ``note`` below)\n",
    "\n",
    "``Note:`` If the name is already taken in the Kaggle then please use a similar pseudo name and add a note in your submission with the name you have used in the Kaggle. \n",
    "\n",
    "#### Submitting your test result to Kaggle leaderboard \n",
    "The class Kaggle competition also includes a blind test set, which will be used in Question 1 for evaluating your custom model's performance on a test set. The competition website will compute the test set accuracy, as well as position your model on the class leaderboard. More information is provided in the related section below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required packages\n",
    "\n",
    "[1] [numpy](http://www.numpy.org) is package for scientific computing with python\n",
    "\n",
    "[2] [h5py](http://www.h5py.org) is package to interact with compactly stored dataset\n",
    "\n",
    "[3] [matplotlib](http://matplotlib.org) can be used for plotting graphs in python\n",
    "\n",
    "[4] [pytorch](https://pytorch.org/docs/stable/index.html) is library widely used for bulding deep-learning frameworks\n",
    "\n",
    "Feel free to add to this section as needed some examples for importing some libraries is provided for you below.\n",
    "\n",
    "You may need to install these packages using [pip](https://pypi.org/project/opencv-python/) or [conda](https://anaconda.org/conda-forge/opencv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import math\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# always check your version\n",
    "print(torch.__version__)\n",
    "\n",
    "if not torch.backends.mps.is_available():\n",
    "    if not torch.backends.mps.is_built():\n",
    "        print(\"MPS not available because the current PyTorch install was not \"\n",
    "              \"built with MPS enabled.\")\n",
    "    else:\n",
    "        print(\"MPS not available because the current MacOS version is not 12.3+ \"\n",
    "              \"and/or you do not have an MPS-enabled device on this machine.\")\n",
    "\n",
    "else:\n",
    "    mps_device = torch.device(\"mps\")\n",
    "\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfR--uYXHdIi"
   },
   "source": [
    "One challenge of building a deep learning model is to choose an architecture that can learn the features in the dataset without being unnecessarily complex. The first part of the coursework involves building a CNN and training it on TinyImageNet30. \n",
    "\n",
    "### **Overview of image classification:**\n",
    "\n",
    "**1. Function implementation** [14 marks]\n",
    "\n",
    "*   **1.1** PyTorch ```Dataset``` and ```DataLoader``` classes (4 marks)\n",
    "*   **1.2** PyTorch ```Model``` class for a simple MLP model (4 marks)\n",
    "*   **1.3** PyTorch ```Model``` class for a simple CNN model (6 marks)\n",
    "\n",
    "**2. Model training** [30 marks]\n",
    "*   **2.1** Training on TinyImageNet30 dataset (6 marks)\n",
    "*   **2.2** Generating confusion matrices and ROC curves (6 marks)\n",
    "*   **2.3** Strategies for tackling overfitting (18 marks)\n",
    "    *   **2.3.1** Data augmentation\n",
    "    *   **2.3.2** Dropout\n",
    "    *   **2.3.3** Hyperparameter tuning (e.g. changing learning rate)\n",
    "            \n",
    "**3. Model testing** [10 marks]\n",
    "*   **3.1**   Testing your final model in (2) on test set - code to do this (4 marks)\n",
    "*   **3.2**   Uploading your result to Kaggle  (6 marks)\n",
    "\n",
    "**4. Model Fine-tuning on CIFAR10 dataset** [16 marks]\n",
    "*   **4.1** Fine-tuning your model (initialise your model with pretrained weights from (2)) (6 marks)\n",
    "*   **4.2** Fine-tuning model with frozen base convolution layers (6 marks)\n",
    "*   **4.3** Compare complete model retraining with pretrained weights and with frozen layers. Comment on what you observe. (4 marks) \n",
    "\n",
    "\n",
    "<!-- **5. Model comparison** [16 marks]\n",
    "*   **5.1**   Load pretrained AlexNet and finetune on TinyImageNet30 until model convergence (8 marks)\n",
    "*   **5.2**   Compare the results of your model with pretrained AlexNet on the same validation set. Provide performance values (loss graph, confusion matrix, top-1 accuracy, execution time) (8 marks) -->\n",
    "<!-- \n",
    "**6. Interpretation of results** (14 marks)\n",
    "*   **6.1** Implement grad-CAM for your model and AlexNet (6 marks)\n",
    "*   **6.2** Visualise and compare your results from your model and AlexNet (4 marks)\n",
    "*   **6.3** Provide comment on (4 marks)\n",
    "    - why the network predictions were correct or not correct in your predictions? \n",
    "    - what can you do to improve your results further?\n",
    "\n",
    "**7. Residual connection for deeper network** (9 marks)\n",
    "*   **7.1** Implement a few residual layers in AlexNet and retrain on TinyImageNet30. You can change network size if you wish. (6 marks)\n",
    "*   **7.2** Comment on why such connections are important and why this impacted your results in terms of loss and accuracy (if it did!) (3 marks)\n",
    "\n",
    "**Quality of your report** (2 marks) -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Function implementations [14 marks]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dataset class (4 marks)\n",
    "\n",
    "Write a PyTorch ```Dataset``` class (an example [here](https://www.askpython.com/python-modules/pytorch-custom-datasets) for reference) which loads the TinyImage30 dataset and ```DataLoaders``` for training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training set images: 10816\n",
      "Number of validation set images: 2752\n"
     ]
    }
   ],
   "source": [
    "# Get the current working directory\n",
    "ROOT = os.getcwd() + \"/\"\n",
    "DATA_DIR = ROOT + \"train_set/train_set/\"\n",
    "\n",
    "# List of class labels\n",
    "CLASS_TXT_LABELS = ['baboon', 'banana', 'bee', 'bison', 'bee', 'bison', 'butterfly', 'candle', 'cardigan', 'chihuahua', \n",
    "                    'elephant', 'espresso', 'fly', 'goldfish', 'goose', 'grasshopper', 'hourglass', 'icecream', 'ipod',\n",
    "                    'jellyfish', 'koala', 'ladybug', 'lion', 'mushroom', 'penguin', 'pig', 'pizza', 'pretzel', 'redpanda',\n",
    "                    'refrigerator', 'sombrero', 'umbrella']\n",
    "\n",
    "# Creating a dataset class for tinyimage30\n",
    "class LoadTinyImage30TrainSet(ImageFolder):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root=DATA_DIR,\n",
    "        transform=transforms.ToTensor()\n",
    "    ):\n",
    "        super(LoadTinyImage30TrainSet, self).__init__(root=root, transform=transform)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def tinyimageshow(self, x):\n",
    "        xa = np.transpose(x.numpy(), (1, 2, 0))\n",
    "        plt.imshow(xa)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        return xa\n",
    "\n",
    "train_set = LoadTinyImage30TrainSet()\n",
    "validation_set = LoadTinyImage30TrainSet()\n",
    "\n",
    "# plot an image to test\n",
    "images, _ = next(iter(train_set))\n",
    "train_set.tinyimageshow(images)\n",
    "\n",
    "# Handling the splitting of train and validation sets\n",
    "num_train = len(train_set)\n",
    "indices = list(range(num_train))\n",
    "# 20 percent used for validation\n",
    "split = int(np.floor(0.2 * num_train)) \n",
    "\n",
    "np.random.seed(0) \n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx, validation_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "validation_sampler = SubsetRandomSampler(validation_idx)\n",
    "\n",
    "\n",
    "# Adjust appropriately depending on GPU resources available / size of problem to speed up training\n",
    "BATCH_SIZE = 64\n",
    "# Adjust appropriately depending on CPU architecture to speed-up loading\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "\n",
    "\n",
    "# The training set\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, \n",
    "                          num_workers=NUM_WORKERS, sampler=train_sampler)\n",
    "\n",
    "# Validation set\n",
    "validation_loader = DataLoader(validation_set, batch_size=BATCH_SIZE,  \n",
    "                               num_workers=NUM_WORKERS, sampler=validation_sampler)\n",
    "\n",
    "# Confirming the whole dataset is loaded in\n",
    "print(\"Number of training set images: \" + str(len(train_loader) * BATCH_SIZE))\n",
    "print(\"Number of validation set images: \" + str(len(validation_loader) * BATCH_SIZE))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define a MLP model class (4 marks)\n",
    "\n",
    "<u>Create a new model class using a combination of:</u>\n",
    "- Input Units\n",
    "- Hidden Units\n",
    "- Output Units\n",
    "- Activation functions\n",
    "- Loss function\n",
    "- Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(\n",
      "  (fc1): Linear(in_features=12288, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=30, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define a MLP Model class https://pytorch.org/vision/main/generated/torchvision.ops.MLP.html\n",
    "#Reminder tiny image 30 has 30 class labels and is made up of 64 *64 images with 3 color channels \n",
    "\n",
    "class MLPClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=30):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(3 * 64 * 64, 120) \n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84,30)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.flatten(x, 1) \n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "mlp_net = MLPClassifier()\n",
    "mlp_loss_fn = nn.CrossEntropyLoss()\n",
    "mlp_optimizer = optim.SGD(mlp_net.parameters(), lr=0.01, momentum=0.9)\n",
    "print(mlp_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Define a CNN model class (6 marks)\n",
    "\n",
    "<u>Create a new model class using a combination of:</u>\n",
    "- Convolution layers\n",
    "- Activation functions (e.g. ReLU)\n",
    "- Maxpooling layers\n",
    "- Fully connected layers \n",
    "- Loss function\n",
    "- Optimiser\n",
    "\n",
    "*Please note that the network should be at least a few layers for the model to perform well.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNClassifier(\n",
      "  (conv1): Conv2d(3, 40, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(40, 80, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=15680, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=30, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define a CNN Model class\n",
    "#https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_classes=30):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3,out_channels=40,kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=40,out_channels=80,kernel_size=3)\n",
    "        self.fc1 = nn.Linear(15680, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 30)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "cnn_net = CNNClassifier()\n",
    "cnn_loss_fn = nn.CrossEntropyLoss()\n",
    "cnn_optimizer = optim.SGD(cnn_net.parameters(), lr=0.01, momentum=0.9)\n",
    "print(cnn_net) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Model training [30 marks]\n",
    "\n",
    "\n",
    "### 2.1 Train both MLP and CNN model - show loss and accuracy graphs side by side (6 marks)\n",
    "\n",
    "Train your model on the TinyImageNet30 dataset. Split the data into train and validation sets to determine when to stop training. Use seed at 0 for reproducibility and test_ratio=0.2 (validation data)\n",
    "\n",
    "Display the graph of training and validation loss over epochs and accuracy over epochs to show how you determined the optimal number of training epochs. A top-*k* accuracy implementation is provided for you below.\n",
    "\n",
    "> Please leave the graph clearly displayed. Please use the same graph to plot graphs for both train and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (HelperDL function) -- Define top-*k* accuracy (**new**)\n",
    "def topk_accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO COMPLETE --> Running your MLP model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO COMPLETE --> Running your CNN model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comment on your model and the results you have obtained. This should include the number of parameters for each of your models and briefly explain why one should use CNN over MLP for the image classification problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Generating confusion matrix and ROC curves (6 marks)\n",
    "- Use your CNN architecture with best accuracy to generate two confusion matrices, one for the training set and another for the validation set. Remember to use the whole validation and training sets, and to include all your relevant code. Display the confusion matrices in a meaningful way which clearly indicates what percentage of the data is represented in each position.\n",
    "- Display an ROC curve for the two top and two bottom classes with area under the curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redesign your CNN model (optional)\n",
    "> This is optional and does not carry any marks. Often to tackle model underfitting we tend to make more complex network design. Depending on your observation, you can improve your model if you wish. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: All questions below here relates to the CNN model only and not an MLP model! You are advised to use your final CNN model only for each of the questions below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Strategies for tackling overfitting (18 marks)\n",
    "Using your (final) CNN model perform the strategies below to avoid overfitting problems. You can resuse the network weights from previous training, often referred to as ``fine tuning``. \n",
    "*   **2.3.1** Data augmentation\n",
    "*   **2.3.2** Dropout\n",
    "*   **2.3.3** Hyperparameter tuning (e.g. changing learning rate)\n",
    "\n",
    "> Plot loss and accuracy graphs per epoch side by side for each implemented strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Data augmentation (6 marks)\n",
    "\n",
    "> Implement at least five different data augmentation techniques that should include both photometric and geometric augmentations. \n",
    "\n",
    "> Provide graphs and comment on what you observe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Dropout (6 marks)\n",
    "\n",
    "> Implement dropout in your model \n",
    "\n",
    "> Provide graphs and comment on your choice of proportion used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Hyperparameter tuning (6 marks)\n",
    "\n",
    "> Use learning rates [0.1, 0.001, 0.0001].\n",
    "\n",
    "> Provide graphs each for loss and accuracy at three different learning rates in a single graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Model testing [10 marks]\n",
    "Online evaluation of your model performance on the test set. \n",
    "\n",
    "> Prepare the dataloader for the testset.\n",
    "\n",
    "> Write evaluation code for writing predictions.\n",
    "\n",
    "> Upload it to Kaggle submission page (6 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 3.1 Test class and predictions (4 marks)\n",
    "\n",
    "> Build a test class, prepare a test dataloader and generate predictions\n",
    "\n",
    "Create a PyTorch ```Dataset``` for the unlabeled test data in the test_set folder of the Kaggle competition and generate predictions using your final model. Test data can be downloaded [here](https://www.kaggle.com/competitions/comp5623m-artificial-intelligence/data?select=test_set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Prepare your submission and upload to Kaggle  (6 marks)\n",
    "\n",
    "Save all test predictions to a CSV file and submit it to the private class Kaggle competition. **Please save your test CSV file submissions using your student username (the one with letters, e.g., ``sc15jb``, not the ID with only numbers)**, for example, `sc15jb.csv`. That will help us to identify your submissions.\n",
    "\n",
    "The CSV file must contain only two columns: ‘Id’ and ‘Category’ (predicted class ID) as shown below:\n",
    "\n",
    "```txt\n",
    "Id,Category\n",
    "28d0f5e9_373c.JPEG,2\n",
    "bbe4895f_40bf.JPEG,18\n",
    "```\n",
    "\n",
    "The ‘Id’ column should include the name of the image. It is important to keep the same name as the one on the test set. Do not include any path, just the name of file (with extension). Your csv file must contain 1501 rows, one for each image on test set and 1 row for the headers. [To submit please click here.](https://www.kaggle.com/t/917fe52f6a3c4855880a24b34f26db07)\n",
    "\n",
    "> You may submit multiple times. We will use your personal top entry for allocating marks for this [6 marks]. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 4 Model Fine-tuning/transfer learning on CIFAR10 dataset  [16 marks]\n",
    "\n",
    "Fine-tuning is a way of applying or utilizing transfer learning. It is a process that takes a model that has already been trained for one given task and then tunes or tweaks the model to make it perform a second similar task. You can perform fine-tuning in following fashion:\n",
    "\n",
    "- Train an entire model: Start training model from scratch (large dataset, more computation)\n",
    "\n",
    "- Train some layers, freeze others: Lower layer features are general (problem independent) while higher layer features are specific (problem dependent – freeze)\n",
    "\n",
    "- Freeze convolution base and train only last FC layers (small dataset and lower computation) \n",
    "\n",
    "> **Configuring your dataset**\n",
    "   - Download your dataset using ``torchvision.datasets.CIFAR10`` [explained here](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html)\n",
    "   - Split training dataset into training and validation set similar to above. *Note that the number of categories here is only 10*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load pretrained AlexNet from PyTorch - use model copies to apply transfer learning in different configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Apply transfer learning with pretrained model weights (6 marks)\n",
    "\n",
    "\n",
    "> Configuration 1: No frozen layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your model changes here - also print trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Fine-tuning model with frozen layers (6 marks)\n",
    "\n",
    "> Configuration 2: Frozen base convolution blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your changes here - also print trainable parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Compare above configurations and comment on performances. (4 marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your graphs here and please provide comment in markdown in another cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: Image Captioning using RNN [30 marks]\n",
    "\n",
    "\n",
    "\n",
    "### Motivation \n",
    "\n",
    "Through this part of assessment you will:\n",
    "\n",
    "> 1. Understand the principles of text pre-processing and vocabulary building (provided).\n",
    "> 2. Gain experience working with an image to text model.\n",
    "> 3. Use and compare text similarity metrics for evaluating an image to text model, and understand evaluation challenges.\n",
    "\n",
    "#### Dataset\n",
    "This assessment will use a subset of the [COCO \"Common Objects in Context\" dataset](https://cocodataset.org/) for image caption generation. COCO contains 330K images, of 80 object categories, and at least five textual reference captions per image. Our subset consists of nearly 5070 of these images, each of which has five or more different descriptions of the salient entities and activities, and we will refer to it as COCO_5070.\n",
    "\n",
    "To download the data:\n",
    "\n",
    "> 1. **Images and annotations**: download the zipped file provided in the link here as [``COMP5625M_data_assessment_2.zip``](https://leeds365-my.sharepoint.com/:u:/g/personal/scssali_leeds_ac_uk/EWWzE-_AIrlOkvOKxH4rjIgBF_eUx8KDJMPKM2eHwCE0dg?e=DdX62H). \n",
    "\n",
    "``Info only:`` To understand more about the COCO dataset you can look at the [download page](https://cocodataset.org/#download). We have already provided you with the \"2017 Train/Val annotations (241MB)\" but our image subset consists of fewer images compared to orginial COCO dataset. So, no need to download anything from here! \n",
    "\n",
    "> 2. **Image meta data**: as our set is a subset of full COCO dataset, we have created a CSV file containing relevant meta data for our particular subset of images. You can download it also from Drive, \"coco_subset_meta.csv\" at the same link as 1.\n",
    "\n",
    "#### Submission\n",
    "\n",
    "You can either submit the same file or make a two separate .ipython notebook files zipped in the submission (please name as ``yourstudentusername_partI.ipynb`` and ``yourstudentusername_partII.ipynb``). \n",
    "\n",
    "**Final note:**\n",
    "\n",
    "> **Please include in this notebook everything that you would like to be marked, including figures. Under each section, put the relevant code containing your solution. You may re-use functions you defined previously, but any new code must be in the relevant section.** Feel free to add as many code cells as you need under each section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic principle of our image-to-text model is as pictured in the diagram below, where an Encoder network encodes the input image as a feature vector by providing the outputs of the last fully-connected layer of a pre-trained CNN (we use [ResNet50](https://arxiv.org/abs/1512.03385)). This pretrained network has been trained on the complete ImageNet dataset and is thus able to recognise common objects. \n",
    "\n",
    "You can alternatively use the COCO trained pretrained weights from [PyTorch](https://pytorch.org/vision/stable/models.html). One way to do this is use the \"FasterRCNN_ResNet50_FPN_V2_Weights.COCO_V1\" but use e.g., \"resnet_model = model.backbone.body\". Alternatively, you can use the checkpoint from your previous coursework where you fine-tuned to COCO dataset. \n",
    "\n",
    "These features are then fed into a Decoder network along with the reference captions. As the image feature dimensions are large and sparse, the Decoder network includes a linear layer which downsizes them, followed by *a batch normalisation layer to speed up training*. Those resulting features, as well as the reference text captions, are then passed into a recurrent network (we will use an **RNNs** in this assessment). \n",
    "\n",
    "The reference captions used to compute loss are represented as numerical vectors via an **embedding layer** whose weights are learned during training.\n",
    "\n",
    "<!-- ![Encoder Decoder](comp5625M_figure.jpg) -->\n",
    "\n",
    "<div>\n",
    "<center><img src=\"comp5625M_figure.jpg\" width=\"1000\"/></center>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions for creating vocabulary \n",
    "\n",
    "A helper function file ``helperDL.py`` is provided that includes all the functions that will do the following for you. You can easily import these functions in the exercise, most are already done for you!  \n",
    "\n",
    "> 1. Extracting image features (a trained checkpoint is provided ``resnet50_caption.pt`` for you to download and use it for training your RNN)\n",
    "> 2. Text preparation of training and validation data is provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Please refer to the submission section at the top of this notebook to prepare your submission.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the feature map provided to you\n",
    "features_map = torch.load('coco_features_'+(device.type)+'.pt', map_location=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 Train DecoderRNN [20 marks]\n",
    "\n",
    "> 5.1 Design a RNN-based decoder (10 marks)\n",
    "\n",
    "> 5.2 Train your model with precomputed features (10 Marks)\n",
    "\n",
    "##### 5.1 Design a RNN-based decoder (10 marks)\n",
    "\n",
    "Read through the ```DecoderRNN``` model below. First, complete the decoder by adding an ```rnn``` layer to the decoder where indicated, using [the PyTorch API as reference](https://pytorch.org/docs/stable/nn.html#rnn).\n",
    "\n",
    "Keep all the default parameters except for ```batch_first```, which you may set to True.\n",
    "\n",
    "In particular, understand the meaning of ```pack_padded_sequence()``` as used in ```forward()```. Refer to the [PyTorch ```pack_padded_sequence()``` documentation](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('coco/annotations2017/captions_train2017.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    \n",
    "df = pd.DataFrame.from_dict(data[\"annotations\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_subset = pd.read_csv(\"coco_subset_meta.csv\")\n",
    "new_data = pd.DataFrame( data['annotations'])\n",
    "new_coco = coco_subset.rename(columns={'id':'image_id'})\n",
    "new_coco.drop_duplicates('file_name',keep='first',inplace=True)\n",
    "\n",
    "new_subset = new_data.sort_values(['image_id'], ascending=True)\n",
    "# Get all the reference captions\n",
    "new_file = pd.merge(new_coco,new_subset,how = 'inner', on = 'image_id')\n",
    "new_file = new_file[['image_id','id','caption','file_name']]\n",
    "new_file = new_file.sort_values(['image_id'], ascending = True)\n",
    "new_file.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the clearn clean - e.g., converting all uppercases to lowercases\n",
    "new_file[\"clean_caption\"] = \"\"\n",
    "from helperDL import gen_clean_captions_df\n",
    "new_file = gen_clean_captions_df(new_file)\n",
    "new_file.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Spilt your training, validation and test dataset with indexes to each set\n",
    "from helperDL import split_ids\n",
    "train_id, valid_id, test_id = split_ids(new_file['image_id'].unique())\n",
    "print(\"training:{}, validation:{}, test:{}\".format(len(train_id), len(valid_id), len(test_id)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = new_file.loc[new_file['image_id'].isin(train_id)]\n",
    "valid_set = new_file.loc[new_file['image_id'].isin(valid_id)]\n",
    "test_set = new_file.loc[new_file['image_id'].isin(test_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary(object):\n",
    "    \"\"\" Simple vocabulary wrapper which maps every unique word to an integer ID. \"\"\"\n",
    "    def __init__(self):\n",
    "        # intially, set both the IDs and words to dictionaries with special tokens\n",
    "        self.word2idx = {'<pad>': 0, '<unk>': 1, '<end>': 2}\n",
    "        self.idx2word = {0: '<pad>', 1: '<unk>', 2: '<end>'}\n",
    "        self.idx = 3\n",
    "\n",
    "    def add_word(self, word):\n",
    "        # if the word does not already exist in the dictionary, add it\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            # increment the ID for the next word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        # if we try to access a word not in the dictionary, return the id for <unk>\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### build vocabulariy for each set - train, val and test \n",
    "# you will be using to create dataloaders\n",
    "from helperDL import build_vocab\n",
    "\n",
    "# create a vocab instance\n",
    "vocab = Vocabulary()\n",
    "vocab_train = build_vocab(train_id, new_file, vocab)\n",
    "vocab_valid = build_vocab(valid_id, new_file, vocab)\n",
    "vocab_test = build_vocab(test_id, new_file, vocab)\n",
    "\n",
    "vocab = vocab_train # using only training samples as vocabulary as instructed above\n",
    "print(\"Total vocabulary size: {}\".format(len(vocab_train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# They can also join the train and valid captions but they will need to run vocabulary after concatenation\n",
    "import numpy as np\n",
    "vocab = build_vocab(np.concatenate((train_id, valid_id), axis=0), new_file, vocab) #---> overrighting\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a ```DataLoader``` for your image feature and caption dataset. ``helperDL.py`` file includes all the required functions\n",
    "\n",
    "We need to overwrite the default PyTorch collate_fn() because our \n",
    "ground truth captions are sequential data of varying lengths. The default\n",
    "collate_fn() does not support merging the captions with padding.\n",
    "\n",
    "You can read more about it here:\n",
    "https://pytorch.org/docs/stable/data.html#dataloader-collate-fn. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helperDL import EncoderCNN  \n",
    "model = EncoderCNN() \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Load the saved image feature maps and trained model (\"provided to you\") '''\n",
    "\n",
    "# ---> your entry here (make sure that the path is correct)\n",
    "features = torch.load(\"coco_features.pt\")\n",
    "\n",
    "# also load the model ckpt and udate the model state dict of the base model\n",
    "# ---> your entry here (make sure that the path is correct)\n",
    "checkpoint = torch.load(\"ckpt file here\") \n",
    "model.load_state_dict(checkpoint['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Preparing the train, val and test dataloaders\n",
    "from helperDL import COCO_Features\n",
    "from helperDL import caption_collate_fn\n",
    "\n",
    "\n",
    "# Create a dataloader for train\n",
    "dataset_train = COCO_Features(\n",
    "    df=train_set,\n",
    "    vocab=vocab,\n",
    "    features=features_map,\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0, # may need to set to 0\n",
    "    collate_fn=caption_collate_fn, # explicitly overwrite the collate_fn\n",
    ")\n",
    "\n",
    "# Create a dataloader for valid\n",
    "dataset_valid = COCO_Features(\n",
    "    df=valid_set,\n",
    "    vocab=vocab,\n",
    "    features=features_map,\n",
    ")\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    dataset_valid,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0, # may need to set to 0\n",
    "    collate_fn=caption_collate_fn, # explicitly overwrite the collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# say this is as below \n",
    "# --> Please change these numbers as required. \n",
    "# --> Please comment on changes that you do.\n",
    "EMBED_SIZE = 256\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 1\n",
    "LR = 0.001\n",
    "NUM_EPOCHS = 5\n",
    "LOG_STEP = 10\n",
    "MAX_SEQ_LEN = 37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Design a RNN-based decoder (10 marks)\n",
    "\n",
    "Read through the ```DecoderRNN``` model below. First, complete the decoder by adding an ```rnn``` layer to the decoder where indicated, using [the PyTorch API as reference](https://pytorch.org/docs/stable/nn.html#rnn).\n",
    "\n",
    "Keep all the default parameters except for ```batch_first```, which you may set to True.\n",
    "\n",
    "In particular, understand the meaning of ```pack_padded_sequence()``` as used in ```forward()```. Refer to the [PyTorch ```pack_padded_sequence()``` documentation](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=256, hidden_size=512, num_layers=1, max_seq_length=47):\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        # we want a specific output size, which is the size of our embedding, so\n",
    "        # we feed our extracted features from the last fc layer (dimensions 1 x 2048)\n",
    "        # into a Linear layer to resize\n",
    "        # your code\n",
    "        \n",
    "        # batch normalisation helps to speed up training\n",
    "        # your code\n",
    "\n",
    "\n",
    "        # your code for embedding layer\n",
    "   \n",
    "\n",
    "        # your code for RNN\n",
    "   \n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "\n",
    "    def forward(self, features, captions, lengths):\n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        embeddings = self.embed(captions)\n",
    "        im_features = self.resize(features)\n",
    "        im_features = self.bn(im_features)\n",
    "        \n",
    "        # compute your feature embeddings\n",
    "        # your code\n",
    "\n",
    "    \n",
    "        # pack_padded_sequence returns a PackedSequence object, which contains two items: \n",
    "        # the packed data (data cut off at its true length and flattened into one list), and \n",
    "        # the batch_sizes, or the number of elements at each sequence step in the batch.\n",
    "        # For instance, given data [a, b, c] and [x] the PackedSequence would contain data \n",
    "        # [a, x, b, c] with batch_sizes=[2,1,1].\n",
    "\n",
    "        # your code [hint: use pack_padded_sequence]\n",
    "    \n",
    "\n",
    "\n",
    "        outputs = self.linear() #hint: use a hidden layers in parenthesis\n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "    def sample(self, features, states=None):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        sampled_ids = []\n",
    "\n",
    "        inputs = self.bn(self.resize(features)).unsqueeze(1)\n",
    "        for i in range(self.max_seq_length):\n",
    "            hiddens, states = self.rnn(inputs, states)  # hiddens: (batch_size, 1, hidden_size)\n",
    "            outputs = self.linear(hiddens.squeeze(1))   # outputs:  (batch_size, vocab_size)\n",
    "            _, predicted = outputs.max(1)               # predicted: (batch_size)\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs = self.embed(predicted)              # inputs: (batch_size, embed_size)\n",
    "            inputs = inputs.unsqueeze(1)                # inputs: (batch_size, 1, embed_size)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)       # sampled_ids: (batch_size, max_seq_length)\n",
    "        return sampled_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate decoder\n",
    "decoder = DecoderRNN(len(vocab), embed_size=EMBED_SIZE, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2 Train your model with precomputed features (10 marks)\n",
    "\n",
    "Train the decoder by passing the features, reference captions, and targets to the decoder, then computing loss based on the outputs and the targets. Note that when passing the targets and model outputs to the loss function, the targets will also need to be formatted using ```pack_padded_sequence()```.\n",
    "\n",
    "We recommend a batch size of around 64 (though feel free to adjust as necessary for your hardware).\n",
    "\n",
    "**We strongly recommend saving a checkpoint of your trained model after training so you don't need to re-train multiple times.**\n",
    "\n",
    "Display a graph of training and validation loss over epochs to justify your stopping point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loss and optimizer here\n",
    "# your code here --->\n",
    "\n",
    "\n",
    "\n",
    "# train the models\n",
    "total_step = len(train_loader)\n",
    "total_step_v = len(valid_loader)\n",
    "stats = np.zeros((NUM_EPOCHS,2))\n",
    "print(stats.shape)\n",
    "total_loss = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for i, (features_, captions_, lengths_) in enumerate(train_loader):\n",
    "        # your code here --->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # print stats\n",
    "        if i % LOG_STEP == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Step [{i}/{total_step}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    stats[epoch,0] = round(total_loss/total_step,3)\n",
    "    total_loss = 0\n",
    "    decoder.eval()\n",
    "    with torch.no_grad():  \n",
    "        for i, (features_, captions_, lengths_) in enumerate(valid_loader):\n",
    "            # your code here --->\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "    stats[epoch,1] = round(total_loss/total_step_v,3)\n",
    "    total_loss = 0\n",
    "    # print stats\n",
    "    print(\"=\"*30)\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Train_Loss: {stats[epoch,0]}, Valid_Loss: {stats[epoch,1]}\")\n",
    "    print(\"=\"*30)\n",
    "    decoder.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "\n",
    "plt.plot(stats[:,0], 'r', label = 'training', )\n",
    "plt.plot(stats[:,1], 'g', label = 'validation' )\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title(f\"COCO dataset- train vocab only #vocab={len(vocab)} - 5 epochs\")\n",
    "fig.savefig(\"coco_train_vocab_only.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model after training\n",
    "decoder_ckpt = torch.save(decoder, \"coco_subset_assessment_decoder.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Test prediction and evaluation [10 marks] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.1 Generate predictions on test data (4 marks)\n",
    "\n",
    "Display 4 sample test images containing different objects, along with your model’s generated captions and all the reference captions for each.\n",
    "\n",
    "> Remember that everything **displayed** in the submitted notebook and .html file will be marked, so be sure to run all relevant cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([ \n",
    "    transforms.Resize(224),     \n",
    "    transforms.CenterCrop(224), \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),   # using ImageNet norms\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "dataset_test = COCOImagesDataset(\n",
    "    df=test_set,\n",
    "    transform=data_transform,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "decoder.eval() # generate caption, eval mode to not influence batchnormncoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting functions from helperDL.py\n",
    "from helperDL import timshow\n",
    "from helperDL import decode_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_TO_SHOW = 4\n",
    "idx = 0\n",
    "with torch.no_grad():\n",
    "    for i, (image,filename) in enumerate(test_loader):\n",
    "        \n",
    "        # your code here --->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        print(f\"GENERATED: \\n\")\n",
    "        print(\"REFERENCES:\")\n",
    "\n",
    "        print(\"===================================\\n\")\n",
    "\n",
    "\n",
    "        timshow(image[0].cpu())\n",
    "        idx +=1\n",
    "        if idx == IMAGES_TO_SHOW:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2 Caption evaluation using cosine similarity (6 marks)\n",
    "\n",
    "The cosine similarity measures the cosine of the angle between two vectors in n-dimensional space. The smaller the angle, the greater the similarity.\n",
    "\n",
    "To use the cosine similarity to measure the similarity between the generated caption and the reference captions: \n",
    "\n",
    "* Find the embedding vector of each word in the caption \n",
    "* Compute the average vector for each caption \n",
    "* Compute the cosine similarity score between the average vector of the generated caption and average vector of each reference caption\n",
    "* Compute the average of these scores \n",
    "\n",
    "Calculate the cosine similarity using the model's predictions over the whole test set. \n",
    "\n",
    "Display a histogram of the distribution of scores over the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Thank you for completing the assessment - if you have any question, please ask on teams channel or attend lab sessions on Tuesdays and Wednesdays."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "COMP5623M_CW1_Q2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
